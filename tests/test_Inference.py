""" Create artificial test data or use the Wine Quality Dataset from the UCI Machine Learning Repository to test the
Inference class. The Inference class is used to train and test different regression models (e.g., Ridge, MLPRegressor,
SNPE) for parameter estimation.
 """

import os
import sys

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# ccpi toolbox
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
import ccpi

def cohen_d(x, y):
    """ Compute Cohen's d effect size.

    Compute Cohen's d effect size for two independent samples.

    Parameters
    ----------
    x : array-like
        First sample.
    y : array-like
        Second sample.

    Returns
    -------
    d : float
        Cohen's d effect size.
    """

    nx = len(x)
    ny = len(y)
    dof = nx + ny - 2
    return (np.mean(x) - np.mean(y)) / np.sqrt(((nx-1)*np.var(x) + (ny-1)*np.var(y)) / dof)

def create_artificial_data(dataset_type=0, n_samples=1000):
    """ Create artificial data for testing.

    Create a Pandas DataFrame with the following columns: 'ID', 'Group', 'Epoch', 'Sensor' and 'Data', and the following
    attributes: 'Recording' and 'fs'. The 'ID' column contains the unique identifier of the subject or animal. The
    'Group' column defines the group in which the subject/animal has been classified: for example, 'Control' or 'AD'.
    The 'Epoch' column contains the epoch number. The 'Sensor' column contains the sensor number. The 'Data' column
    contains the time-series data values. The 'Recording' attribute contains the type of recording (e.g., 'LFP') and
    the 'fs' attribute contains the sampling frequency of the data.

    Parameters
    ----------
    dataset_type: int
        Type of dataset to generate. The following types are available:
        - 0: Generate a dataset with time-series data.
        - 1: Use the Wine Quality Dataset from the UCI Machine Learning Repository.
    n_samples : int
        Number of samples to generate (only used for dataset_type=0).

    Returns
    -------
    df : Pandas DataFrame
        Pandas DataFrame containing the artificial data
    """

    if dataset_type == 0:
        # Create a list of unique identifiers (there is a total of 10 subjects/animals).
        ID = np.repeat(np.arange(1, 11), int(n_samples/10))
        # Create a list of groups. There are two groups: 'Control' and 'Experiment'.
        Group = np.repeat(['Control', 'Experiment'], int(n_samples/2))
        # Create a list of epochs. There are n_samples/10 epochs for each subject/animal.
        Epoch = np.tile(np.arange(1, int(n_samples/10)+1), 10)
        # Create a list of sensors (there is just one sensor).
        Sensor = np.ones(n_samples)

        # Create a list of time-series data values. The data values are generated by convolving a Poisson spike train with
        # an exponential kernel, and adding Gaussian noise. The length of the time-series data is 1000 samples.
        Data = []
        for i in range(n_samples):
            spike_train = np.random.poisson(0.1, 1500)
            if Group[i] == 'Control':
                kernel = np.exp(-np.arange(0, 1500)/10.0)
            elif Group[i] == 'Experiment':
                kernel = np.exp(-np.arange(0, 1500)/20.0)
            data = np.convolve(spike_train, kernel, mode='full')[250:1250] + np.random.normal(0, 0.1, 1000)
            Data.append(data)

    elif dataset_type == 1:
        # Download the Wine Quality Dataset
        print('Downloading the Wine Quality Dataset...')
        wq_df = pd.read_csv(
            'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',
                         sep=';')
        # There are as many subjects as samples in the Wine Quality Dataset
        ID = np.arange(1, wq_df.shape[0]+1)
        # There are two groups: 'Control' (Quality < 6.) and 'Experiment' (Quality >= 6.)
        Group = np.where(wq_df['quality'] < 6., 'Control', 'Experiment')
        # There is just one epoch per subject
        Epoch = np.ones(wq_df.shape[0])
        # There is just one sensor
        Sensor = np.ones(wq_df.shape[0])
        # The data values are the features of the Wine Quality Dataset (excluding the 'quality' column)
        Data = wq_df.drop('quality', axis=1).values
        # Transform Data to a list of lists
        Data = [list(Data[i]) for i in range(Data.shape[0])]

    # Create the Pandas DataFrame.
    df = pd.DataFrame({'ID': ID, 'Group': Group, 'Epoch': Epoch, 'Sensor': Sensor, 'Data': Data})
    df.Recording = 'LFP'
    df.fs = 100.0 # Hz

    return df


if __name__ == '__main__':
    # Create the dataset
    data = create_artificial_data(dataset_type=0,n_samples=5000)

    # Compute features
    if len(data['Data'][0]) > 100:
        features = ccpi.Features(method='catch22')
        data = features.compute_features(data)
    # If the data is already in feature format (Wine Quality Dataset), just rename the 'Data' column to 'Features'
    else:
        data['Features'] = data['Data']

    # # Plot a random feature as a function of the group
    # rand_feat = np.random.randint(0, len(data['Features'][0]))
    # rand_feat_pd = pd.DataFrame({'Group': data['Group'],
    #                              'Feature': data['Features'].apply(lambda x: x[rand_feat])})
    # sns.boxplot(x='Group', y='Feature', data=rand_feat_pd)
    # plt.title(f'Feature {rand_feat}')
    # plt.show()

    # Split data into training and testing sets
    train_data = data.sample(frac=0.9)
    test_data = data.drop(train_data.index)

    # Sort data by index
    train_data = train_data.sort_index()
    test_data = test_data.sort_index()

    # Assume that the 'Group' column can be interpreted as the parameter to be estimated
    le = LabelEncoder()
    parameters = le.fit_transform(train_data['Group'])

    # Test different regression models
    models = ['Ridge', 'MLPRegressor','SNPE']
    # Hyperparameters to be optimized
    hyperparams_opt = {'Ridge': [{'alpha': 0.1}, {'alpha': 1.0}, {'alpha': 10.0}, {'alpha': 100.0}],
                       'MLPRegressor':
                           [{'hidden_layer_sizes': (25,), 'max_iter': 1000, 'tol': 1e-2, 'n_iter_no_change': 4},
                            {'hidden_layer_sizes': (50,), 'max_iter': 1000, 'tol': 1e-2, 'n_iter_no_change': 4},
                            {'hidden_layer_sizes': (100,), 'max_iter': 1000, 'tol': 1e-2, 'n_iter_no_change': 4},
                            {'hidden_layer_sizes': (25,25), 'max_iter': 1000, 'tol': 1e-2, 'n_iter_no_change': 4},
                            {'hidden_layer_sizes': (50,50), 'max_iter': 1000, 'tol': 1e-2, 'n_iter_no_change': 4},
                            {'hidden_layer_sizes': (100,100), 'max_iter': 1000, 'tol': 1e-2, 'n_iter_no_change': 4},
                            {'hidden_layer_sizes': (25,25,25), 'max_iter': 1000, 'tol': 1e-2, 'n_iter_no_change': 4},
                            {'hidden_layer_sizes': (50,50,50), 'max_iter': 1000, 'tol': 1e-2, 'n_iter_no_change': 4},
                            {'hidden_layer_sizes': (100,100,100), 'max_iter': 1000, 'tol': 1e-2, 'n_iter_no_change': 4}],
                       'SNPE':
                           [{'prior': None, 'density_estimator': {
                               'model':"maf", 'hidden_features':2, 'num_transforms':1}},
                            {'prior': None, 'density_estimator': {
                                'model':"maf", 'hidden_features':5, 'num_transforms':1}},
                            {'prior': None, 'density_estimator': {
                                'model':"maf", 'hidden_features':10, 'num_transforms':1}}
                            ]}
    predictions = {}

    for model in models:
        print(f'\nTraining {model}...')

        for mode in ['no_param_grid', 'param_grid']:
            print(f'\nRunning {model} with {mode}...')

            if mode == 'no_param_grid':
                hyperparams = hyperparams_opt[model][0]
            else:
                hyperparams = None

            # Create the Inference object and add the train data
            inference = ccpi.Inference(model=model, hyperparams=hyperparams)
            inference.add_simulation_data(np.array(train_data['Features'].tolist()), parameters)

            # Train the model
            if mode == 'no_param_grid':
                inference.train(param_grid=None)
            else:
                inference.train(param_grid=hyperparams_opt[model], n_splits=5, n_repeats=1)

            # Test the model
            print(f'\nComputing predictions for {model}...')
            predictions[model+'_'+mode] = inference.predict(np.array(test_data['Features'].tolist()))

    # Plot results
    plt.figure(dpi = 300)
    plt.rc('font', size=8)
    plt.rc('font', family='Arial')

    for i,model in enumerate(models):
        for j,label in enumerate(['no_param_grid', 'param_grid']):
            plt.subplot(2, len(models), i + 1 + j*len(models))
            sns.boxplot(x='Group', y='Prediction', data=pd.DataFrame({'Group': test_data['Group'],
                                                                      'Prediction': predictions[model+'_'+label]}),
                        palette='Set2', legend=False, hue='Group')
            # Plot cohen-d
            d = cohen_d(np.array(predictions[model+'_'+label])[test_data['Group'] == 'Control'],
                        np.array(predictions[model+'_'+label])[test_data['Group'] == 'Experiment'])
            plt.text(0.5, 0.9, f'Cohen\'s d = {d:.2f}',
                     horizontalalignment='center',
                     verticalalignment='center',
                        transform=plt.gca().transAxes)
            # Title and labels
            plt.title(f'{model} ({label})')
            plt.ylabel('')
            plt.xlabel('')
            plt.xticks(rotation=45)
            plt.tight_layout()

            if i == 0:
                plt.ylabel('Prediction')

            # Increase y-axis limits to show the Cohen's d value
            ylim = plt.gca().get_ylim()
            plt.gca().set_ylim(ylim[0], ylim[1] * 1.2)

    plt.show()
