""" This script generates artificial electrophysiological data and tests the functionality of the Inference class. """

import os
import sys
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# ncpi toolbox
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
import ncpi

def cohen_d(x, y):
    """ Compute Cohen's d effect size.

    Compute Cohen's d effect size for two independent samples.

    Parameters
    ----------
    x : array-like
        First sample.
    y : array-like
        Second sample.

    Returns
    -------
    d : float
        Cohen's d effect size.
    """

    nx = len(x)
    ny = len(y)
    dof = nx + ny - 2
    return (np.mean(x) - np.mean(y)) / np.sqrt(((nx-1)*np.var(x) + (ny-1)*np.var(y)) / dof)

def create_artificial_data(n_samples=1000, len_data=1000):
    """ Generate artificial data with dynamic properties resembling those of electrophysiological signals
    for testing purposes. Create a Pandas DataFrame with the following columns: 'ID', 'Group', 'Epoch', 'Sensor' and
    'Data', and the following attributes: 'Recording' and 'fs'. The 'ID' column contains the unique identifier of the
    subject or animal. The 'Group' column defines the group in which the subject/animal has been classified: for example,
    'Control' or 'AD'. The 'Epoch' column contains the epoch number. The 'Sensor' column contains the sensor identifier.
    The 'Data' column contains the time-series data values. The 'Recording' attribute contains the type of recording
    (e.g., 'LFP' or 'EEG') and the 'fs' attribute contains the sampling frequency of the data.

    Parameters
    ----------
    n_samples : int
        Number of samples to generate.
    len_data : int
        Length of the time-series data.

    Returns
    -------
    df : Pandas DataFrame
        Pandas DataFrame containing the artificial data
    """

    # Create a list of unique identifiers. In this example, there is a total of 10 subjects/animals.
    ID = np.repeat(np.arange(1, 11), int(n_samples/10))
    # Create a list of groups. There are two groups: 'Control' and 'Experiment'.
    Group = np.repeat(['Control', 'Experiment'], int(n_samples/2))
    # Create a list of epochs. There are n_samples/10 epochs for each subject/animal.
    Epoch = np.tile(np.arange(1, int(n_samples/10)+1), 10)
    # Create a list of sensors (there is just one sensor).
    Sensor = np.ones(n_samples)

    # Create a list of time-series data values. The data values are generated by convolving a Poisson spike train with
    # an exponential kernel, and adding Gaussian noise.
    Data = []
    for i in range(n_samples):
        spike_train = np.random.poisson(0.1, len_data+500)
        if Group[i] == 'Control':
            kernel = np.exp(-np.arange(0, len_data+500)/10.0)
        elif Group[i] == 'Experiment':
            kernel = np.exp(-np.arange(0, len_data+500)/14.0)
        data = (np.convolve(spike_train, kernel, mode='full')[250:len_data+250] +
                np.random.normal(0, 0.1, len_data))
        Data.append(data)

    # Create the Pandas DataFrame.
    df = pd.DataFrame({'ID': ID, 'Group': Group, 'Epoch': Epoch, 'Sensor': Sensor, 'Data': Data})
    df.Recording = 'LFP'
    df.fs = 100.0 # Hz

    return df


if __name__ == '__main__':
    # Set the seed
    np.random.seed(0)

    # Create the dataset
    data = create_artificial_data(n_samples=2000, len_data=1000)

    # Compute features
    features = ncpi.Features(method='catch22')
    data = features.compute_features(data)

    # Plot all features as a function of the group
    plt.figure(dpi=300)
    plt.rc('font', size=8)
    plt.rc('font', family='Arial')

    for i in range(len(data['Features'][0])):
        plt.subplot(5, 5, i + 1)
        sns.boxplot(x='Group', y='Feature', data=pd.DataFrame({'Group': data['Group'],
                                                               'Feature': data['Features'].apply(lambda x: x[i])}),
                    palette='Set2', legend=False, hue='Group')
        plt.title(f'Feature {i}')
        plt.gca().set_xticklabels([])
        plt.xlabel('')
        plt.ylabel('')
        plt.tight_layout()

    # Split data into training and testing sets
    train_data = data.sample(frac=0.9)
    test_data = data.drop(train_data.index)

    # Sort data by index
    train_data = train_data.sort_index()
    test_data = test_data.sort_index()

    # Assume that the 'Group' column can be interpreted as the parameter to be estimated
    le = LabelEncoder()
    parameters = le.fit_transform(train_data['Group'])

    # Test different regression models
    models = ['Ridge', 'MLPRegressor','SNPE']
    # Hyperparameters to be optimized
    hyperparams_opt = {'Ridge': [{'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1.}, {'alpha': 10.}, {'alpha': 100.}],
                       'MLPRegressor':
                           [{'hidden_layer_sizes': (25, 25, 25), 'max_iter': 1000, 'tol': 1e-2},
                            {'hidden_layer_sizes': (50, 50, 50), 'max_iter': 1000, 'tol': 1e-2},
                            {'hidden_layer_sizes': (100, 100, 100), 'max_iter': 1000, 'tol': 1e-2}],
                       'SNPE':
                           [{'prior': None, 'density_estimator': {'model': "maf", 'hidden_features': 2,
                                                                  'num_transforms': 1}},
                            {'prior': None, 'density_estimator': {'model': "maf", 'hidden_features': 2,
                                                                  'num_transforms': 2}}
                            ]}
    predictions = {}

    for model in models:
        print(f'\nTraining {model}...')

        for mode in ['no_param_grid', 'param_grid']:
            print(f'\nRunning {model} with {mode}...')

            if mode == 'no_param_grid':
                hyperparams = hyperparams_opt[model][0]
            else:
                hyperparams = None

            # Create the Inference object and add the train data
            inference = ncpi.Inference(model=model, hyperparams=hyperparams)
            inference.add_simulation_data(np.array(train_data['Features'].tolist()), parameters)

            # Train the model
            if mode == 'no_param_grid':
                inference.train(param_grid=None)
            else:
                inference.train(param_grid=hyperparams_opt[model], n_splits=5, n_repeats=1)

            # Compute predictions
            print(f'\nComputing predictions for {model}...')
            preds = inference.predict(np.array(test_data['Features'].tolist()))
            if model == 'SNPE':
                predictions[model+'_'+mode] = [preds[i][0] for i in range(len(preds))]
            else:
                predictions[model+'_'+mode] = preds

    # Plot results
    plt.figure(dpi = 300)

    for i,model in enumerate(models):
        for j,label in enumerate(['no_param_grid', 'param_grid']):
            plt.subplot(2, len(models), i + 1 + j*len(models))
            sns.boxplot(x='Group', y='Prediction', data=pd.DataFrame({'Group': test_data['Group'],
                                                                      'Prediction': predictions[model+'_'+label]}),
                        palette='Set2', legend=False, hue='Group')
            # Plot cohen-d
            d = cohen_d(np.array(predictions[model+'_'+label])[test_data['Group'] == 'Control'],
                        np.array(predictions[model+'_'+label])[test_data['Group'] == 'Experiment'])
            plt.text(0.5, 0.9, f'Cohen\'s d = {d:.2f}',
                     horizontalalignment='center',
                     verticalalignment='center',
                        transform=plt.gca().transAxes)

            plt.title(f'{model} ({label})')
            plt.ylabel('')
            plt.xlabel('')
            plt.xticks(rotation=45)
            plt.tight_layout()

            if i == 0:
                plt.ylabel('Prediction')

            # Increase y-axis limits to show the Cohen's d value
            ylim = plt.gca().get_ylim()
            plt.gca().set_ylim(ylim[0], ylim[1] * 1.2)

    plt.show()
